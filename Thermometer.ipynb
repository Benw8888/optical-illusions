{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6fcf70-9d16-49c5-a16f-eb4132cf8c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "import jax\n",
    "jax.config.update(\"jax_debug_nans\", True)\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24da1605-0783-462c-b466-840f1687fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import razor\n",
    "\n",
    "class model(razor.Model):\n",
    "    def forward(self, t, m):  # here, \"t\" is the \"trace\" object used by the PPL's compiler\n",
    "        T = t.gaussian_sample((), 70., 5)\n",
    "        t.gaussian_observe(T, 2., m)\n",
    "model = model(0.)  # 0. is a dummy value for m, just used for shape inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1b1431-2436-4674-9b89-556ca63b83cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[T | M=100] ~= 95.95879364013672\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q0 = np.zeros(model.N)\n",
    "key = jax.random.PRNGKey(0)\n",
    "samples = razor.hmc_sample(\n",
    "    model, q0, key, 1000, 300, 0, 1e-2,  # hyperparameters\n",
    "    100.  # value of M to condition on\n",
    ")\n",
    "plt.hist(samples[100:, 0], bins=20); plt.title('Distribution of T | M = 100')\n",
    "print(f'E[T | M=100] ~= {samples[100:, 0].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb33df3-4d2d-47d5-a537-b85d387c9a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874e6c17d34c4e9b9caeecb67bb9e8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0: E[T | M=  0.000] ~=   9.595 (loss = 8173.0127)\n",
      "Step  1: E[T | M= 15.587] ~=  23.053 (loss = 5920.8770)\n",
      "Step  2: E[T | M= 28.854] ~=  34.511 (loss = 4288.8687)\n",
      "Step  3: E[T | M= 40.145] ~=  44.147 (loss = 3119.5610)\n",
      "Step  4: E[T | M= 49.775] ~=  52.580 (loss = 2248.6382)\n",
      "Step  5: E[T | M= 57.950] ~=  59.614 (loss = 1630.9949)\n",
      "Step  6: E[T | M= 64.913] ~=  65.585 (loss = 1184.3597)\n",
      "Step  7: E[T | M= 70.847] ~=  70.731 (loss =  856.6827)\n",
      "Step  8: E[T | M= 75.893] ~=  75.094 (loss =  620.3095)\n",
      "Step  9: E[T | M= 80.188] ~=  78.922 (loss =  444.2906)\n",
      "Step 10: E[T | M= 83.822] ~=  81.859 (loss =  329.0795)\n",
      "Step 11: E[T | M= 86.950] ~=  84.592 (loss =  237.3998)\n",
      "Step 12: E[T | M= 89.606] ~=  86.905 (loss =  171.4823)\n",
      "Step 13: E[T | M= 91.864] ~=  88.772 (loss =  126.0592)\n",
      "Step 14: E[T | M= 93.800] ~=  90.447 (loss =   91.2534)\n",
      "Step 15: E[T | M= 95.447] ~=  91.966 (loss =   64.5433)\n",
      "Step 16: E[T | M= 96.832] ~=  93.105 (loss =   47.5423)\n",
      "Step 17: E[T | M= 98.021] ~=  94.152 (loss =   34.1955)\n",
      "Step 18: E[T | M= 99.029] ~=  94.956 (loss =   25.4371)\n",
      "Step 19: E[T | M= 99.898] ~=  95.878 (loss =   16.9900)\n",
      "Step 20: E[T | M=100.609] ~=  96.345 (loss =   13.3561)\n",
      "Step 21: E[T | M=101.239] ~=  96.870 (loss =    9.7977)\n",
      "Step 22: E[T | M=101.779] ~=  97.434 (loss =    6.5867)\n",
      "Step 23: E[T | M=102.221] ~=  97.727 (loss =    5.1670)\n",
      "Step 24: E[T | M=102.613] ~=  98.137 (loss =    3.4704)\n",
      "Step 25: E[T | M=102.934] ~=  98.485 (loss =    2.2944)\n",
      "Step 26: E[T | M=103.196] ~=  98.650 (loss =    1.8217)\n",
      "Step 27: E[T | M=103.428] ~=  98.819 (loss =    1.3937)\n",
      "Step 28: E[T | M=103.632] ~=  99.034 (loss =    0.9326)\n",
      "Step 29: E[T | M=103.798] ~=  99.168 (loss =    0.6929)\n",
      "Step 30: E[T | M=103.942] ~=  99.259 (loss =    0.5485)\n",
      "Step 31: E[T | M=104.070] ~=  99.334 (loss =    0.4435)\n",
      "Step 32: E[T | M=104.184] ~=  99.567 (loss =    0.1875)\n",
      "Step 33: E[T | M=104.259] ~=  99.532 (loss =    0.2190)\n",
      "Step 34: E[T | M=104.340] ~=  99.564 (loss =    0.1904)\n",
      "Step 35: E[T | M=104.415] ~=  99.599 (loss =    0.1604)\n",
      "Step 36: E[T | M=104.484] ~=  99.746 (loss =    0.0645)\n",
      "Step 37: E[T | M=104.528] ~=  99.681 (loss =    0.1020)\n",
      "Step 38: E[T | M=104.583] ~=  99.832 (loss =    0.0282)\n",
      "Step 39: E[T | M=104.612] ~=  99.810 (loss =    0.0362)\n",
      "Step 40: E[T | M=104.645] ~=  99.864 (loss =    0.0184)\n",
      "Step 41: E[T | M=104.668] ~=  99.976 (loss =    0.0006)\n",
      "Step 42: E[T | M=104.672] ~=  99.938 (loss =    0.0039)\n",
      "Step 43: E[T | M=104.683] ~=  99.877 (loss =    0.0152)\n",
      "Step 44: E[T | M=104.704] ~= 100.027 (loss =    0.0007)\n",
      "Step 45: E[T | M=104.699] ~=  99.883 (loss =    0.0137)\n",
      "Step 46: E[T | M=104.720] ~= 100.042 (loss =    0.0018)\n",
      "Step 47: E[T | M=104.712] ~=  99.789 (loss =    0.0443)\n",
      "Step 48: E[T | M=104.749] ~=  99.970 (loss =    0.0009)\n",
      "Step 49: E[T | M=104.754] ~=  99.839 (loss =    0.0259)\n",
      "Step 50: E[T | M=104.782] ~= 100.052 (loss =    0.0027)\n",
      "Step 51: E[T | M=104.773] ~= 100.044 (loss =    0.0019)\n",
      "Step 52: E[T | M=104.765] ~= 100.025 (loss =    0.0006)\n",
      "Step 53: E[T | M=104.761] ~= 100.064 (loss =    0.0041)\n",
      "Step 54: E[T | M=104.750] ~=  99.970 (loss =    0.0009)\n",
      "Step 55: E[T | M=104.755] ~=  99.874 (loss =    0.0160)\n",
      "Step 56: E[T | M=104.777] ~=  99.952 (loss =    0.0024)\n",
      "Step 57: E[T | M=104.785] ~=  99.867 (loss =    0.0176)\n",
      "Step 58: E[T | M=104.808] ~=  99.945 (loss =    0.0030)\n",
      "Step 59: E[T | M=104.817] ~=  99.963 (loss =    0.0014)\n",
      "Step 60: E[T | M=104.824] ~= 100.172 (loss =    0.0298)\n",
      "Step 61: E[T | M=104.794] ~=  99.939 (loss =    0.0038)\n",
      "Step 62: E[T | M=104.805] ~=  99.976 (loss =    0.0006)\n",
      "Step 63: E[T | M=104.809] ~= 100.181 (loss =    0.0328)\n",
      "Step 64: E[T | M=104.778] ~=  99.899 (loss =    0.0103)\n",
      "Step 65: E[T | M=104.795] ~=  99.873 (loss =    0.0160)\n",
      "Step 66: E[T | M=104.817] ~= 100.030 (loss =    0.0009)\n",
      "Step 67: E[T | M=104.812] ~= 100.001 (loss =    0.0000)\n",
      "Step 68: E[T | M=104.812] ~=  99.917 (loss =    0.0069)\n",
      "Step 69: E[T | M=104.826] ~= 100.015 (loss =    0.0002)\n",
      "Step 70: E[T | M=104.823] ~= 100.060 (loss =    0.0036)\n",
      "Step 71: E[T | M=104.813] ~=  99.951 (loss =    0.0024)\n",
      "Step 72: E[T | M=104.821] ~= 100.011 (loss =    0.0001)\n",
      "Step 73: E[T | M=104.819] ~=  99.950 (loss =    0.0025)\n",
      "Step 74: E[T | M=104.828] ~=  99.960 (loss =    0.0016)\n",
      "Step 75: E[T | M=104.835] ~= 100.047 (loss =    0.0022)\n",
      "Step 76: E[T | M=104.827] ~= 100.086 (loss =    0.0073)\n",
      "Step 77: E[T | M=104.812] ~=  99.971 (loss =    0.0008)\n",
      "Step 78: E[T | M=104.817] ~= 100.009 (loss =    0.0001)\n",
      "Step 79: E[T | M=104.815] ~= 100.076 (loss =    0.0058)\n",
      "Step 80: E[T | M=104.802] ~=  99.974 (loss =    0.0007)\n",
      "Step 81: E[T | M=104.807] ~=  99.870 (loss =    0.0170)\n",
      "Step 82: E[T | M=104.829] ~= 100.177 (loss =    0.0315)\n",
      "Step 83: E[T | M=104.799] ~= 100.029 (loss =    0.0008)\n",
      "Step 84: E[T | M=104.794] ~= 100.002 (loss =    0.0000)\n",
      "Step 85: E[T | M=104.793] ~=  99.997 (loss =    0.0000)\n",
      "Step 86: E[T | M=104.794] ~= 100.071 (loss =    0.0051)\n",
      "Step 87: E[T | M=104.782] ~=  99.861 (loss =    0.0192)\n",
      "Step 88: E[T | M=104.805] ~= 100.058 (loss =    0.0034)\n",
      "Step 89: E[T | M=104.795] ~=  99.957 (loss =    0.0018)\n",
      "Step 90: E[T | M=104.803] ~=  99.968 (loss =    0.0010)\n",
      "Step 91: E[T | M=104.808] ~=  99.916 (loss =    0.0070)\n",
      "Step 92: E[T | M=104.823] ~=  99.979 (loss =    0.0004)\n",
      "Step 93: E[T | M=104.826] ~= 100.045 (loss =    0.0020)\n",
      "Step 94: E[T | M=104.819] ~= 100.038 (loss =    0.0014)\n",
      "Step 95: E[T | M=104.812] ~= 100.061 (loss =    0.0037)\n",
      "Step 96: E[T | M=104.802] ~=  99.939 (loss =    0.0038)\n",
      "Step 97: E[T | M=104.812] ~= 100.053 (loss =    0.0028)\n",
      "Step 98: E[T | M=104.803] ~=  99.996 (loss =    0.0000)\n",
      "Step 99: E[T | M=104.804] ~= 100.078 (loss =    0.0061)\n"
     ]
    }
   ],
   "source": [
    "# Loss function. q0 is the initial particle state for HMC.\n",
    "def loss(m, key, q0):\n",
    "    samples = razor.hmc_sample(model, q0, key, 1000, 300, 0, 1e-2, m)\n",
    "    samples = samples[100:]\n",
    "    mu = samples.mean()\n",
    "    return (mu - 100.0) ** 2, samples\n",
    "dloss = jax.value_and_grad(loss, argnums=0, has_aux=True)\n",
    "\n",
    "# Optimization loop\n",
    "key = jax.random.PRNGKey(0)\n",
    "m = np.array(0.)  # initial value for gradient descent\n",
    "for i in tqdm(range(100)):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    q0 = jax.random.normal(key=subkey, shape=(model.N,))\n",
    "    key, subkey = jax.random.split(key)\n",
    "    (l, s), dl = dloss(m, subkey, q0)\n",
    "    print(f'Step {i:2d}: E[T | M={m:7.03f}] ~= {s.mean():7.03f} (loss = {l:9.4f})')\n",
    "    m = m - 0.1 * dl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
